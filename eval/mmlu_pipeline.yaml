# PIPELINE DEFINITION
# Name: mmlu-eval
# Description: Pipeline to run the MMLU evaluation script
# Inputs:
#    batch_size: int [Default: 8.0]
#    device: str [Default: 'None']
#    few_shots: int [Default: 5.0]
#    mmlu_tasks_list: str [Default: 'mmlu_abstract_algebra,mmlu_anatomy,mmlu_astronomy']
#    model_dtype: str [Default: 'bfloat16']
components:
  comp-importer:
    executorLabel: exec-importer
    inputDefinitions:
      parameters:
        uri:
          parameterType: STRING
    outputDefinitions:
      artifacts:
        artifact:
          artifactType:
            schemaTitle: system.Model
            schemaVersion: 0.0.1
  comp-load-mmlu-results:
    executorLabel: exec-load-mmlu-results
    inputDefinitions:
      artifacts:
        mmlu_output:
          artifactType:
            schemaTitle: system.Artifact
            schemaVersion: 0.0.1
    outputDefinitions:
      parameters:
        Output:
          parameterType: STRUCT
  comp-run-mmlu:
    executorLabel: exec-run-mmlu
    inputDefinitions:
      artifacts:
        candidate_model:
          artifactType:
            schemaTitle: system.Model
            schemaVersion: 0.0.1
      parameters:
        batch_size:
          parameterType: NUMBER_INTEGER
        device:
          parameterType: STRING
        few_shots:
          parameterType: NUMBER_INTEGER
        mmlu_tasks_list:
          parameterType: STRING
        model_dtype:
          parameterType: STRING
    outputDefinitions:
      artifacts:
        mmlu_output_file:
          artifactType:
            schemaTitle: system.Artifact
            schemaVersion: 0.0.1
deploymentSpec:
  executors:
    exec-importer:
      importer:
        artifactUri:
          constant: s3://sallyom-eval-e58df6b0-606b-4749-96a5-a105657cb068/models/instructlab/granite-7b-lab
        typeSchema:
          schemaTitle: system.Model
          schemaVersion: 0.0.1
    exec-load-mmlu-results:
      container:
        args:
        - --executor_input
        - '{{$}}'
        - --function_to_execute
        - load_mmlu_results
        command:
        - sh
        - -c
        - "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip ||\
          \ python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1\
          \ python3 -m pip install --quiet --no-warn-script-location 'kfp==2.8.0'\
          \ '--no-deps' 'typing-extensions>=3.7.4,<5; python_version<\"3.9\"' && \"\
          $0\" \"$@\"\n"
        - sh
        - -ec
        - 'program_path=$(mktemp -d)


          printf "%s" "$0" > "$program_path/ephemeral_component.py"

          _KFP_RUNTIME=true python3 -m kfp.dsl.executor_main                         --component_module_path                         "$program_path/ephemeral_component.py"                         "$@"

          '
        - "\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import *\nfrom typing import\
          \ *\n\ndef load_mmlu_results(mmlu_output: Input[Artifact]) -> dict:\n  \
          \  import json\n\n    with open(mmlu_output.path, 'r') as file:\n      \
          \  evaluation_data = json.load(file)\n\n    print(\"Loaded Evaluation Data:\"\
          )\n    print(json.dumps(evaluation_data, indent=4))\n\n    return evaluation_data\n\
          \n"
        image: registry.access.redhat.com/ubi9/toolbox
    exec-run-mmlu:
      container:
        args:
        - --executor_input
        - '{{$}}'
        - --function_to_execute
        - run_mmlu
        command:
        - sh
        - -c
        - "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip ||\
          \ python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1\
          \ python3 -m pip install --quiet --no-warn-script-location 'kfp==2.8.0'\
          \ '--no-deps' 'typing-extensions>=3.7.4,<5; python_version<\"3.9\"' && \"\
          $0\" \"$@\"\n"
        - sh
        - -ec
        - 'program_path=$(mktemp -d)


          printf "%s" "$0" > "$program_path/ephemeral_component.py"

          _KFP_RUNTIME=true python3 -m kfp.dsl.executor_main                         --component_module_path                         "$program_path/ephemeral_component.py"                         "$@"

          '
        - "\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import *\nfrom typing import\
          \ *\n\ndef run_mmlu(\n    candidate_model: Input[Model],\n    mmlu_output_file:\
          \ Output[Artifact],\n    mmlu_tasks_list: str,\n    model_dtype: str,\n\
          \    few_shots: int,\n    batch_size: int,\n    device: str,\n):\n    import\
          \ json\n    import os\n    import torch\n    from instructlab.eval.mmlu\
          \ import MMLUEvaluator, MMLU_TASKS\n\n    model_name = os.path.basename(os.path.normpath(candidate_model.path))\n\
          \    # Debug\n    print(f\"Model {model_name} is stored at: {candidate_model.path}\"\
          )\n\n    mmlu_tasks = mmlu_tasks_list.split(',') if mmlu_tasks_list else\
          \ MMLU_TASKS\n\n    # Device setup and debug\n    gpu_available = torch.cuda.is_available()\n\
          \    gpu_name = torch.cuda.get_device_name(torch.cuda.current_device())\
          \ if gpu_available else \"No GPU available\"\n\n    print(f\"GPU Available:\
          \ {gpu_available}, Using: {gpu_name}\")\n\n    effective_device = device\
          \ if device != \"None\" else (\"cuda\" if gpu_available else \"cpu\")\n\
          \    print(f\"Running on device: {effective_device}\")\n\n    # Evaluation\n\
          \    evaluator = MMLUEvaluator(\n        model_path=candidate_model.path,\n\
          \        tasks=mmlu_tasks,\n        model_dtype=model_dtype,\n        few_shots=few_shots,\n\
          \        batch_size=batch_size,\n        device=effective_device,\n    )\n\
          \n    mmlu_score, individual_scores = evaluator.run()\n    mmlu_data = {\n\
          \        \"report_title\": \"KNOWLEDGE EVALUATION REPORT\",\n        \"\
          model\": model_name,\n        \"average_score\": round(mmlu_score, 2),\n\
          \        \"number_of_tasks\": len(individual_scores),\n        \"individual_scores\"\
          : [{task: round(score['score'], 2)} for task, score in individual_scores.items()]\n\
          \    }\n\n    with open(mmlu_output_file.path, 'w') as f:\n        json.dump(mmlu_data,\
          \ f, indent=4)\n\n"
        image: quay.io/sallyom/instructlab-ocp:eval
        resources:
          accelerator:
            count: '1'
            type: nvidia.com/gpu
pipelineInfo:
  description: Pipeline to run the MMLU evaluation script
  displayName: MMLU Evaluation Pipeline
  name: mmlu-eval
root:
  dag:
    tasks:
      importer:
        cachingOptions:
          enableCache: true
        componentRef:
          name: comp-importer
        inputs:
          parameters:
            uri:
              runtimeValue:
                constant: s3://sallyom-eval-e58df6b0-606b-4749-96a5-a105657cb068/models/instructlab/granite-7b-lab
        taskInfo:
          name: importer
      load-mmlu-results:
        cachingOptions:
          enableCache: true
        componentRef:
          name: comp-load-mmlu-results
        dependentTasks:
        - run-mmlu
        inputs:
          artifacts:
            mmlu_output:
              taskOutputArtifact:
                outputArtifactKey: mmlu_output_file
                producerTask: run-mmlu
        taskInfo:
          name: load-mmlu-results
      run-mmlu:
        cachingOptions:
          enableCache: true
        componentRef:
          name: comp-run-mmlu
        dependentTasks:
        - importer
        inputs:
          artifacts:
            candidate_model:
              taskOutputArtifact:
                outputArtifactKey: artifact
                producerTask: importer
          parameters:
            batch_size:
              componentInputParameter: batch_size
            device:
              componentInputParameter: device
            few_shots:
              componentInputParameter: few_shots
            mmlu_tasks_list:
              componentInputParameter: mmlu_tasks_list
            model_dtype:
              componentInputParameter: model_dtype
        taskInfo:
          name: run-mmlu
  inputDefinitions:
    parameters:
      batch_size:
        defaultValue: 8.0
        isOptional: true
        parameterType: NUMBER_INTEGER
      device:
        defaultValue: None
        isOptional: true
        parameterType: STRING
      few_shots:
        defaultValue: 5.0
        isOptional: true
        parameterType: NUMBER_INTEGER
      mmlu_tasks_list:
        defaultValue: mmlu_abstract_algebra,mmlu_anatomy,mmlu_astronomy
        isOptional: true
        parameterType: STRING
      model_dtype:
        defaultValue: bfloat16
        isOptional: true
        parameterType: STRING
schemaVersion: 2.1.0
sdkVersion: kfp-2.8.0
