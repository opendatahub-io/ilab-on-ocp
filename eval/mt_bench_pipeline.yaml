# PIPELINE DEFINITION
# Name: mt-bench-eval
# Description: Pipeline to run the MT_BENCH evaluation
# Inputs:
#    device: str
#    max_workers: str [Default: 'auto']
#    merge_system_user_message: bool [Default: False]
components:
  comp-importer:
    executorLabel: exec-importer
    inputDefinitions:
      parameters:
        uri:
          parameterType: STRING
    outputDefinitions:
      artifacts:
        artifact:
          artifactType:
            schemaTitle: system.Model
            schemaVersion: 0.0.1
  comp-load-mt-bench-results:
    executorLabel: exec-load-mt-bench-results
    inputDefinitions:
      artifacts:
        mt_bench_output:
          artifactType:
            schemaTitle: system.Artifact
            schemaVersion: 0.0.1
    outputDefinitions:
      parameters:
        Output:
          parameterType: STRUCT
  comp-run-mt-bench:
    executorLabel: exec-run-mt-bench
    inputDefinitions:
      parameters:
        candidate_model_name:
          parameterType: STRING
        candidate_server_url:
          parameterType: STRING
        device:
          parameterType: STRING
        max_workers:
          parameterType: STRING
        merge_system_user_message:
          parameterType: BOOLEAN
    outputDefinitions:
      artifacts:
        mt_bench_output:
          artifactType:
            schemaTitle: system.Artifact
            schemaVersion: 0.0.1
  comp-serve-candidate-model:
    executorLabel: exec-serve-candidate-model
    inputDefinitions:
      artifacts:
        candidate_model:
          artifactType:
            schemaTitle: system.Model
            schemaVersion: 0.0.1
    outputDefinitions:
      parameters:
        candidate_model_name:
          parameterType: STRING
        candidate_server_url:
          parameterType: STRING
deploymentSpec:
  executors:
    exec-importer:
      importer:
        artifactUri:
          constant: s3://sallyom-eval-e58df6b0-606b-4749-96a5-a105657cb068/models/instructlab/granite-7b-lab
        typeSchema:
          schemaTitle: system.Model
          schemaVersion: 0.0.1
    exec-load-mt-bench-results:
      container:
        args:
        - --executor_input
        - '{{$}}'
        - --function_to_execute
        - load_mt_bench_results
        command:
        - sh
        - -c
        - "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip ||\
          \ python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1\
          \ python3 -m pip install --quiet --no-warn-script-location 'kfp==2.8.0'\
          \ '--no-deps' 'typing-extensions>=3.7.4,<5; python_version<\"3.9\"' && \"\
          $0\" \"$@\"\n"
        - sh
        - -ec
        - 'program_path=$(mktemp -d)


          printf "%s" "$0" > "$program_path/ephemeral_component.py"

          _KFP_RUNTIME=true python3 -m kfp.dsl.executor_main                         --component_module_path                         "$program_path/ephemeral_component.py"                         "$@"

          '
        - "\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import *\nfrom typing import\
          \ *\n\ndef load_mt_bench_results(mt_bench_output: Input[Artifact]) -> dict:\n\
          \    import json\n\n    with open(mt_bench_output.path, 'r') as file:\n\
          \        mt_bench_data = json.load(file)\n    print(\"Loaded MT_Bench Data:\"\
          )\n    print(json.dumps(mt_bench_data, indent=4))\n\n    return mt_bench_data\n\
          \n"
        image: registry.access.redhat.com/ubi9/toolbox
    exec-run-mt-bench:
      container:
        args:
        - --executor_input
        - '{{$}}'
        - --function_to_execute
        - run_mt_bench
        command:
        - sh
        - -c
        - "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip ||\
          \ python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1\
          \ python3 -m pip install --quiet --no-warn-script-location 'kfp==2.8.0'\
          \ '--no-deps' 'typing-extensions>=3.7.4,<5; python_version<\"3.9\"' && \"\
          $0\" \"$@\"\n"
        - sh
        - -ec
        - 'program_path=$(mktemp -d)


          printf "%s" "$0" > "$program_path/ephemeral_component.py"

          _KFP_RUNTIME=true python3 -m kfp.dsl.executor_main                         --component_module_path                         "$program_path/ephemeral_component.py"                         "$@"

          '
        - "\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import *\nfrom typing import\
          \ *\n\ndef run_mt_bench(\n    candidate_server_url: str,\n    candidate_model_name:\
          \ str,\n    mt_bench_output: Output[Artifact],\n    device: str,\n    max_workers:\
          \ str,\n    merge_system_user_message: bool,\n):\n    import json\n    import\
          \ torch\n    import os\n    #from instructlab.eval.mt_bench import MTBenchEvaluator\n\
          \    from instructlab.eval import mt_bench_answers, mt_bench_judgment\n\n\
          \    os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\
          \n\n    gpu_available = torch.cuda.is_available()\n    gpu_name = torch.cuda.get_device_name(torch.cuda.current_device())\
          \ if gpu_available else \"No GPU available\"\n\n    print(f\"GPU Available:\
          \ {gpu_available}, Using: {gpu_name}\")\n\n    effective_device = device\
          \ if device is not None else (\"cuda\" if gpu_available else \"cpu\")\n\n\
          \    print(f\"Running on device: {effective_device}\")\n\n    judge_model_name\
          \ = os.getenv(\"JUDGE_MODEL_NAME\")\n    judge_endpoint = os.getenv(\"JUDGE_MODEL_ENDPOINT\"\
          )\n    print(f\"Candidate Server URL: {candidate_server_url}\")\n    print(f\"\
          Candidate Model Name: {candidate_model_name}\")\n\n    if max_workers ==\
          \ \"auto\":\n        try:\n            usable_cpu_count = len(os.sched_getaffinity(0))\
          \ // 2\n        except AttributeError:\n            usable_cpu_count = multiprocessing.cpu_count()\
          \ // 2\n        max_workers = usable_cpu_count\n\n    # TODO: Using evaluator\
          \ results in connection errors, need to determine why.\n    #       For\
          \ now, using mt_bench_answers.generate_answers & mt_bench_judgment.generate_judgment\n\
          \    #evaluator = MTBenchEvaluator(\n    #    model_name=candidate_model_name,\n\
          \    #    judge_model_name=judge_model_name,\n    #    max_workers=max_workers,\n\
          \    #    merge_system_user_message=merge_system_user_message\n    #)\n\n\
          \    judge_api_key = os.getenv(\"JUDGE_API_KEY\", \"\")\n\n    print(\"\
          Generating answers...\")\n    # TODO: launch vLLM with candidate-model\n\
          \    # Faking it by using judge_endpoint for generate, so need to pass the\
          \ api_key & candidate_server_url is actually judge_server_url for now\n\
          \    # Need to add a launch_server task with https://github.com/instructlab/instructlab/blob/main/src/instructlab/model/evaluate.py#L301C1-L327C1\n\
          \    # with local vLLM, won't need to pass an api_key here\n    #\n    #\
          \ See note above about issue with evaluator\n    #evaluator.gen_answers(server_url=candidate_server_url,\
          \ api_key=judge_api_key)\n    mt_bench_answers.generate_answers(\n     \
          \   model_name=candidate_model_name,\n        model_api_base=candidate_server_url,\n\
          \        api_key=judge_api_key,\n        output_dir=\"/tmp/eval_output\"\
          ,\n        max_workers=max_workers\n    )\n\n    print(\"Judging answers...\"\
          )\n    # See note above about issue with evaluator\n    #overall_score,\
          \ qa_pairs, turn_scores = evaluator.judge_answers(server_url=judge_endpoint,\
          \ api_key=judge_api_key)\n    overall_score, qa_pairs, turn_scores, error_rate\
          \ = mt_bench_judgment.generate_judgment(\n        model_name = candidate_model_name,\n\
          \        judge_model_name=judge_model_name,\n        model_api_base=judge_endpoint,\n\
          \        api_key=judge_api_key,\n        output_dir=\"/tmp/eval_output\"\
          ,\n        max_workers=max_workers,\n        merge_system_user_message=merge_system_user_message\n\
          \    )\n\n    mt_bench_data = {\n        \"report_title\": \"SKILLS EVALUATION\
          \ REPORT\",\n        \"model\": candidate_model_name,\n        \"judge_model\"\
          : judge_model_name,\n        \"overall_score\": overall_score,\n       \
          \ \"turn_scores\": turn_scores,\n        \"qa_scores\": qa_pairs,\n    \
          \    \"error_rate\": error_rate,\n    }\n\n    with open(mt_bench_output.path,\
          \ 'w') as f:\n        json.dump(mt_bench_data, f, indent=4)\n\n"
        image: quay.io/sallyom/instructlab-ocp:eval
        resources:
          accelerator:
            count: '1'
            type: nvidia.com/gpu
    exec-serve-candidate-model:
      container:
        args:
        - --executor_input
        - '{{$}}'
        - --function_to_execute
        - serve_candidate_model
        command:
        - sh
        - -c
        - "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip ||\
          \ python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1\
          \ python3 -m pip install --quiet --no-warn-script-location 'kfp==2.8.0'\
          \ '--no-deps' 'typing-extensions>=3.7.4,<5; python_version<\"3.9\"' && \"\
          $0\" \"$@\"\n"
        - sh
        - -ec
        - 'program_path=$(mktemp -d)


          printf "%s" "$0" > "$program_path/ephemeral_component.py"

          _KFP_RUNTIME=true python3 -m kfp.dsl.executor_main                         --component_module_path                         "$program_path/ephemeral_component.py"                         "$@"

          '
        - "\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import *\nfrom typing import\
          \ *\n\ndef serve_candidate_model(\n    candidate_model: Input[Model]\n)\
          \ -> NamedTuple('outputs', candidate_model_name=str, candidate_server_url=str):\n\
          \    import os\n\n    candidate_model_name = os.path.basename(os.path.normpath(candidate_model.path))\n\
          \    print(f\"(real)Candidate Model Name: {candidate_model_name}\")\n  \
          \  # TODO: REMOVE after implement vLLM with candidate model\n    # Fake\
          \ model-name for now, with judge_model_name\n    candidate_model_name =\
          \ os.getenv(\"JUDGE_MODEL_NAME\") \n    print(f\"(fake)Candidate Model Name:\
          \ {candidate_model_name}\")\n\n    # TODO: Replace this with the candidate\
          \ model endpoint\n    # Launch_server call with https://github.com/instructlab/instructlab/blob/main/src/instructlab/model/evaluate.py#L301C1-L327C1\n\
          \    candidate_server_url = os.getenv(\"JUDGE_MODEL_ENDPOINT\")\n    outputs\
          \ = NamedTuple('outputs', candidate_model_name=str, candidate_server_url=str)\n\
          \    return outputs(candidate_model_name, candidate_server_url)\n\n"
        image: quay.io/sallyom/instructlab-ocp:eval
pipelineInfo:
  description: Pipeline to run the MT_BENCH evaluation
  displayName: MT_BENCH Evaluation Pipeline
  name: mt-bench-eval
root:
  dag:
    tasks:
      importer:
        cachingOptions:
          enableCache: true
        componentRef:
          name: comp-importer
        inputs:
          parameters:
            uri:
              runtimeValue:
                constant: s3://sallyom-eval-e58df6b0-606b-4749-96a5-a105657cb068/models/instructlab/granite-7b-lab
        taskInfo:
          name: importer
      load-mt-bench-results:
        cachingOptions:
          enableCache: true
        componentRef:
          name: comp-load-mt-bench-results
        dependentTasks:
        - run-mt-bench
        inputs:
          artifacts:
            mt_bench_output:
              taskOutputArtifact:
                outputArtifactKey: mt_bench_output
                producerTask: run-mt-bench
        taskInfo:
          name: load-mt-bench-results
      run-mt-bench:
        cachingOptions:
          enableCache: true
        componentRef:
          name: comp-run-mt-bench
        dependentTasks:
        - serve-candidate-model
        inputs:
          parameters:
            candidate_model_name:
              taskOutputParameter:
                outputParameterKey: candidate_model_name
                producerTask: serve-candidate-model
            candidate_server_url:
              taskOutputParameter:
                outputParameterKey: candidate_server_url
                producerTask: serve-candidate-model
            device:
              componentInputParameter: device
            max_workers:
              componentInputParameter: max_workers
            merge_system_user_message:
              componentInputParameter: merge_system_user_message
        taskInfo:
          name: run-mt-bench
      serve-candidate-model:
        cachingOptions:
          enableCache: true
        componentRef:
          name: comp-serve-candidate-model
        dependentTasks:
        - importer
        inputs:
          artifacts:
            candidate_model:
              taskOutputArtifact:
                outputArtifactKey: artifact
                producerTask: importer
        taskInfo:
          name: serve-candidate-model
  inputDefinitions:
    parameters:
      device:
        isOptional: true
        parameterType: STRING
      max_workers:
        defaultValue: auto
        isOptional: true
        parameterType: STRING
      merge_system_user_message:
        defaultValue: false
        isOptional: true
        parameterType: BOOLEAN
schemaVersion: 2.1.0
sdkVersion: kfp-2.8.0
---
platforms:
  kubernetes:
    deploymentSpec:
      executors:
        exec-run-mt-bench:
          configMapAsEnv:
          - configMapName: kfp-model-server
            keyToEnv:
            - configMapKey: judge_model_endpoint
              envVar: JUDGE_MODEL_ENDPOINT
            - configMapKey: judge_model_name
              envVar: JUDGE_MODEL_NAME
          imagePullPolicy: Always
          secretAsEnv:
          - keyToEnv:
            - envVar: JUDGE_API_KEY
              secretKey: judge_api_key
            secretName: kfp-model-server
        exec-serve-candidate-model:
          configMapAsEnv:
          - configMapName: kfp-model-server
            keyToEnv:
            - configMapKey: judge_model_endpoint
              envVar: JUDGE_MODEL_ENDPOINT
            - configMapKey: judge_model_name
              envVar: JUDGE_MODEL_NAME
